{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TgmJaOh97SBf",
        "jOCF55xg7mDO",
        "CfT9Dztr7ptJ",
        "ai_1o5Ev74vD",
        "VoyZ2GN59jfe",
        "trIAd1So-SbI",
        "v7dWXm_i-Kxx",
        "QMBI8zroCC0u",
        "M2EqguidP0Yl",
        "QRz0jaqVDFgd",
        "Bg6LT7HhDalb",
        "r2ewSgMdEgZ4",
        "qrzPNdxYHc0v"
      ],
      "authorship_tag": "ABX9TyO98v0GP68jhT57IlJjEd1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/A-Mahla/serda-formation-python/blob/main/exercices/google_colab/Bases_du_Client_Python_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bases du Client Python OpenAI"
      ],
      "metadata": {
        "id": "C_T76hoM7QF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction aux API OpenAI"
      ],
      "metadata": {
        "id": "TgmJaOh97SBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qu'est-ce qu'une API ?\n",
        "\n",
        "  - Une API (Interface de Programmation d'Applications) permet à des programmes d'interagir entre eux. Dans le cas d'OpenAI, elle permet aux développeurs d'envoyer des prompts (instructions) et de recevoir des réponses générées par les modèles GPT.\n",
        "\n"
      ],
      "metadata": {
        "id": "CV2weR9Y7VGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Le client Python OpenAI\n",
        "\n",
        "- Le package Python `openai` est un client officiel qui facilite l'accès aux API OpenAI depuis des applications Python. Il gère les détails de la communication réseau, la sérialisation des données, et les bonnes pratiques de sécurité."
      ],
      "metadata": {
        "id": "arg5vhHc7cyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation et Configuration"
      ],
      "metadata": {
        "id": "jOCF55xg7mDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation du package\n",
        "\n",
        "- Assurez-vous que Python est installé.\n",
        "- Installez le package en utilisant pip :\n",
        "\n",
        "```bash\n",
        "pip install openai\n",
        "```"
      ],
      "metadata": {
        "id": "CfT9Dztr7ptJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai"
      ],
      "metadata": {
        "id": "bbM5Uxpz7zCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration de la clé API"
      ],
      "metadata": {
        "id": "ai_1o5Ev74vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- La clé API est essentielle pour authentifier vos requêtes auprès de l'API OpenAI.\n",
        "- Il est recommandé de ne jamais coder en dur votre clé API dans vos scripts. Utilisez plutôt des variables d'environnement :\n",
        "\n",
        "```python\n",
        "import os\n",
        "import openai\n",
        "\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "```"
      ],
      "metadata": {
        "id": "67iRejbc8NFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pour les besoins de ce cours, nous allons démontrer comment utiliser la clé API.\n",
        "# Cependant, il est crucial de ne jamais exposer publiquement votre clé API dans votre code ou sur des plateformes partagées.\n",
        "# Dans un environnement de production, stockez toujours les clés API dans des variables d'environnement ou des services de gestion sécurisée des secrets.\n",
        "\n",
        "openai_api_key = \"votre_clef_api\"  # Remplacez \"votre_clé_api\" par la clé API Openai."
      ],
      "metadata": {
        "id": "cvEM-3TO76W6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Réalisation d'un appel API simple"
      ],
      "metadata": {
        "id": "VoyZ2GN59jfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Création du client OpenAI"
      ],
      "metadata": {
        "id": "trIAd1So-SbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avant de faire une requête, nous devons créer un client OpenAI en utilisant notre clé API. Cela permet d'authentifier nos appels API et de gérer la session de communication avec le service OpenAI."
      ],
      "metadata": {
        "id": "Vz9nJ6t5-x6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key,\n",
        ")"
      ],
      "metadata": {
        "id": "ZIBm5R5X-5nT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Envoyer une requête de complétion de texte"
      ],
      "metadata": {
        "id": "v7dWXm_i-Kxx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Utilisation de la méthode `chat.completions.create()` du client pour demander des complétions de texte au modèle GPT.\n",
        "- Paramètres de base :\n",
        "  - `model`: spécifie le modèle à utiliser, par exemple \"gpt-3.5-turbo-0125\".\n",
        "  - `messages`: Une liste de dictionnaires, chaque dictionnaire représentant un message de l'utilisateur ou du système. Pour conserver le contexte ou fournir des paires de questions-réponses afin de guider le modèle (approche de few-shot), on utilise les rôles user et assistant pour distinguer les questions des réponses.\n",
        "  - `max_tokens`: le nombre maximum de tokens (mots ou morceaux de mots) que la réponse peut contenir."
      ],
      "metadata": {
        "id": "RZaY95xW_P_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Expliquez ce qu'est une API et son utilité.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Pr1bPuXhBmX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Exploration des paramètres avancés"
      ],
      "metadata": {
        "id": "QMBI8zroCC0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `temperature`: contrôle le degré de créativité et de déviation par rapport au prompt. Plus la température est élevée, plus les réponses sont uniques et moins prévisibles.\n",
        "- `top_p`: méthode de sampling qui permet de contrôler la diversité de la réponse générée en limitant la sélection à un sous-ensemble de tokens possibles."
      ],
      "metadata": {
        "id": "ezveSlReCLDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Créez une courte histoire sur un voyage dans l'espace.\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    max_tokens=150,\n",
        "    temperature=0.7,\n",
        "    top_p=1\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "Sy_xjw7dCYDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Généreration en JSON"
      ],
      "metadata": {
        "id": "M2EqguidP0Yl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Une façon courante d'utiliser les complétions de conversation est de demander au modèle de toujours renvoyer un objet JSON qui correspond à votre cas d'utilisation, en spécifiant cela dans le message système. Bien que cela fonctionne dans certains cas, il arrive parfois que les modèles génèrent des sorties qui ne se traduisent pas en objets JSON valides.\n",
        "\n",
        "Vous pouvez définir le format de réponse à { \"type\": \"json_object\" } pour activer le mode JSON. Lorsque le mode JSON est activé, le modèle est contraint de générer uniquement des chaînes qui se traduisent en un objet JSON valide.\n",
        "\n",
        "**Notes importantes :**\n",
        "\n",
        "- Lorsque vous utilisez le mode JSON, demandez toujours explicitement au modèle de produire du JSON via un message dans la conversation, par exemple via votre message système. Si vous n'incluez pas d'instruction explicite pour générer du JSON, le modèle peut générer un flux continu d'espaces blancs et la requête peut continuer indéfiniment jusqu'à atteindre la limite de tokens. Pour vous aider à ne pas oublier, l'API générera une erreur si la chaîne \"JSON\" n'apparaît pas quelque part dans le contexte.\n",
        "- Le JSON dans le message que le modèle renvoie peut être partiel (c'est-à-dire tronqué) si la raison de fin est la longueur, ce qui indique que la génération a dépassé le nombre maximum de tokens ou que la conversation a dépassé la limite de tokens. Pour vous prémunir contre cela, vérifiez la raison de fin avant d'analyser la réponse.\n",
        "- Le mode JSON ne garantit pas que la sortie corresponde à un schéma spécifique, seulement qu'elle est valide et s'analyse sans erreurs.\"\n"
      ],
      "metadata": {
        "id": "NGhbhE-wQFem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  response_format={ \"type\": \"json_object\" },\n",
        "  max_tokens=150,\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
        "  ]\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "BMyI8Z1tRA1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Bonnes pratiques"
      ],
      "metadata": {
        "id": "QRz0jaqVDFgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Gestion des erreurs"
      ],
      "metadata": {
        "id": "Bg6LT7HhDalb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est important de gérer les exceptions et les erreurs réseau lors de l'utilisation de l'API."
      ],
      "metadata": {
        "id": "OqIUkjPYDiXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=\"blablabla\",\n",
        ")\n",
        "\n",
        "try:\n",
        "    completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Racontez une blague sur les programmeurs.\",\n",
        "            }\n",
        "        ],\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        max_tokens=50\n",
        "    )\n",
        "\n",
        "    print(completion.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur est survenue: {e}\")"
      ],
      "metadata": {
        "id": "5Md6MqG1DmvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Optimisation des coûts"
      ],
      "metadata": {
        "id": "r2ewSgMdEgZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les requêtes à l'API sont facturées en fonction du nombre de tokens. Il est donc judicieux d'optimiser vos prompts et de bien choisir le nombre de tokens pour contrôler vos dépenses."
      ],
      "metadata": {
        "id": "8lpss256ElHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice Pratique"
      ],
      "metadata": {
        "id": "qrzPNdxYHc0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Exercice 1: Premier appel API\n",
        "\n",
        "  - Écrivez un script qui demande à l'utilisateur un thème et utilise GPT pour générer ce qu'il vous plaira :)"
      ],
      "metadata": {
        "id": "chEJdSBnHhE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code ...\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bVI0vZZJJWlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Exercice 2: Exploration des paramètres\n",
        "\n",
        "  - Expérimentez avec les paramètres temperature et max_tokens pour observer les variations dans les réponses générées."
      ],
      "metadata": {
        "id": "Hek3Vd7EJbr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code ...\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rhD1qr5aJmaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Exercice 3: Compréhension du contexte\n",
        "  - Le système doit maintenir un contexte de conversation pour que les réponses aux questions suivantes puissent prendre en compte les informations fournies précédemment par l'utilisateur.\n",
        "  - Par exemple, si l'utilisateur demande \"Quelles sont les meilleures destinations en Europe ?\", et suit avec \"Quel est le meilleur moment pour y aller ?\", le système devrait comprendre que la deuxième question se réfère toujours aux destinations européennes."
      ],
      "metadata": {
        "id": "dPbLPajsJoeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Code ...\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eTlGFFyAKFWT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}